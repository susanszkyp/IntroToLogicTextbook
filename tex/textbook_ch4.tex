\chapter{Doing zeroth-order logic}

We now have a cursory acquaintance with the semantics of $\mathcal{L}_0$, so we know how the meaning of formulas are specified, and in particular, how truth-values are assigned to formulas, both atomic and complex. Thus, we are in the position to start talking about \textit{logic}. Now what logic is exactly is hard to say, since by now, the field is vast and intertwined with many other disciplines. At any rate, one of the fundamental aspects of logic is examining how the truth-values of certain sentences relate to the truth-values of certain other sentences. This sounds like a semantic question, since in this formulation, it is about truth-values. On the other hand, over the years, logicians have come up with a myriad of ways to examine these relations without talking about truth-values, or anything else in semantics. To do this, they introduced syntactic deductive systems, which rely purely on some syntactic rules of transformation to tell you whether certain sentences \textit{entailed} other sentences, or in other words, whether those sentences were \textit{logical consequences} of the initial set (among other things). Now since this book likes to be unorthodox, we will be using a deductive system that is technically syntactic in its nature, since it relies on purely syntactic rules for transforming the formulas of the language into other formulas of the language, but it is also highly tied to the semantics we previously examined. The approach is variously called \textit{analytic tableau}, \textit{semantic tableau}, or the \textit{truth tree method}. In fact, in its modern form, it was invented by the logician-mathematician-philosopher-magician-pianist Raymond Smullyan, who was once a CUNY professor! 

\section{Some fundamental concepts of logic}

If you encountered propositional logic before, you may know that there are three large classes of formulas one may distinguish. Usually, these are called \textit{contingent}, \textit{tautology}, and \textit{contradiction}. In our semantics, all this means is the following:

\begin{defn}
A formula $X$ of $\mathcal{L}_0$ is a \textit{tautology} \textit{iff} for \textit{every} structure $\mathbf{S}$, $\mathbf{S} \models X$. A formula $X$ is a \textit{contradiction} \textit{iff} there is \textit{no} structure $\mathbf{S}$ such that $\mathbf{S} \models X$. Finally, a formula $X$ is \textit{contingent} if it is neither a tautology, nor a contradiction. 
\end{defn}

\begin{remark}
Since tautologies are true in every structure, and contradictions are true in no structure, contingent formulas are those that are true in some structures, false in others. 
\end{remark}

Now one thing to note at the beginning is that atomic formulas are all contingent. That is, for each atomic formula $P$, there is a structure $\mathbf{S}$ such that $\mathbf{S} \models P$, and there is a structure $\mathbf{S}'$ such that $\mathbf{S}' \not\models P$. On the other hand, some complex formulas are contingent, some are tautologies, and some are contradictions. 

What makes a formula a tautology or a contradiction? It is the way the connectives work. Notice that by definition, each connective must obey some specific rules in any structure regarding the truth-values of the formulas on which it operates. For example, $X \wedge Y$ is true in any $\mathbf{S}$ \textit{iff} $X$ and $Y$ are both true individually in $\mathbf{S}$. However, some formulas may say things that go against these rules. This means that they won't be true in any structure $\mathsf{S}$, since again, every structure must abide by these rules, while the formula says otherwise. 

Let's take the simplest example: $X \wedge \neg X$. In fact, this is not a formula of the language, but a \textit{class} of formulas of form $X \wedge \neg X$. But as it turns out, no matter what formula you take for $X$, it will always be the case that for any structure $\mathbf{S}$, $\mathbf{S} \not\models X \wedge \neg X$. In other words, this formula schema cannot have a true instance. 

Now again, the reason for this is the way structures are defined. In particular, in every structure $\mathbf{S}$, either $\mathbf{S} \models X$, or $\mathbf{S} \not\models X$. This is sometimes called the \textit{Law of Excluded Middle}, and it falls out of our definition of a structure. What this means is that every formula $X$ is either true in a structure, or false in a structure, and not a third thing (the `excluded middle'). 

We also have that it is never the case that $\mathbf{S} \models X$ and $\mathbf{S} \not\models X$. This is sometimes called the \textit{Law of Non-contradiction}. Again, this falls out of our definition of what it means for something to be a structure. Now adding these two laws together, you simply get that every formula $X$ is either true or false, \textit{and} not both \textit{or} neither in a structure.

So take $X \wedge \neg X$. By the above, either $\mathbf{S} \models X$, or $\mathbf{S} \models \neg X$. Those are the only two options we have. We can consider each of them in turn:

\begin{enumerate}
	\item Suppose $\mathbf{S} \models X$. Then, by definition of $\neg$, $\mathbf{S} \not\models X$. But $\mathbf{S} \models X \wedge \neg X$ \textit{iff} $\mathbf{S} \models X$, \textit{and} $\mathbf{S} \models \neg X$, by definition of $\wedge$. So since the latter half fails, $\mathbf{S} \not\models X \wedge \neg X$. 
	\item Suppose $\mathbf{S} \not\models X$. Then, by definition of $\neg$, $\mathbf{S} \models \neg X$ in this case. But again, $\mathbf{S} \models X \wedge \neg X$ \textit{iff} $\mathbf{S} \models X$, \textit{and} $\mathbf{S} \models \neg X$, by definition of $\wedge$. Moreover, in this case, we already assumed that $\mathbf{S} \not\models X$. So once again, $\mathbf{S} \not\models X \wedge \neg X$. 
\end{enumerate}

This means that there is no structure $\mathbf{S}$ in which $X \wedge \neg X$ is true. So every instance of $X \wedge \neg X$ is false in every structure, no matter what you take $X$ to be. 

At first, this type of talk might be confusing, but all we did was precisely represent what is usually illustrated by the following truth table:

\begin{center}
	\begin{tabular}{c|c|c}
		$X$ & $\neg X$ & $X \wedge \neg X$\\ \hline
		$T$ & $F$ & $F$\\
		$F$ & $T$ & $F$
	\end{tabular}
\end{center}

In particular, a formula is contradictory, as represented in a truth table, if all values in the column underneath the formula are false. Again, the truth table rows just represent two large (jointly exhaustive) classes of structures, one in which $X$ is true, the other in which $X$ is false, and then show that neither of these classes make $X \wedge \neg X$ true. 

\fpbox{It will be important in what's to come that every contradictory formula hides a claim of the form $X \wedge \neg X$ for some formula $X$. In fact, every contradictory formula hides a claim of the form $P \wedge \neg P$, for some \textit{atomic} formula $P$. In other words, the only contradictory formulas are those that try to say that something both is and is not the case (though not necessarily in exactly those words).}

On the other hand, we may find formulas of the form $X \vee \neg X$, which are not contradictory, but the exact opposite, \textit{tautological}. Again, the reason is because of how the connectives are defined in a structure. Again, in every structure, we have that either $\mathbf{S} \models X$, or $\mathbf{S} \not\models X$. So:

\begin{enumerate}
	\item If $\mathbf{S} \models X$, then $\mathbf{S} \models X \vee \neg X$, since by definition of $\vee$, $S \models X \vee \neg X$ \textit{iff} it models either $X$ or $\neg X$.
	\item If $\mathbf{S} \not \models X$, then by definition of $\neg$, $\mathbf{S} \models \neg X$, so by definition of $\vee$, $\mathbf{S} \models X \vee \neg X$ (because of the latter half of the disjunction now). 
\end{enumerate}

Thus, $X \vee \neg X$ must be true in \textit{every} structure $\mathbf{S}$, no matter which formula we take for the variable $X$. And again, this is just an explanation of the following truth-table:

\begin{center}
	\begin{tabular}{c|c|c}
		$X$ & $\neg X$ & $X \vee \neg X$\\ \hline
		$T$ & $F$ & $T$\\
		$F$ & $T$ & $T$
	\end{tabular}
\end{center}

As you can see, unlike with contradictions, tautologies have all values as true in the column underneath them. 

Now we can formulate one of the most significant connections between tautologies and contradictions. As such, we will give it the proper form of a theorem. 

\begin{prop}
A formula $X$ of $\mathcal{L}_0$ is a tautology \textit{iff} its negation $\neg X$ is a contradiction. \label{tautcon}
\end{prop}

\begin{proof}
From left to right, suppose $X$ is a tautology. Then, for every $\mathbf{S}$, $\mathbf{S} \models X$. By definition, $\mathbf{S} \models X$ \textit{iff} $\mathbf{S} \not\models \neg X$. So since \textit{every} $\mathbf{S} \models X$, \textit{no} $\mathbf{S} \models \neg X$. By definition, $\neg X$ is then a contradiction. 

From right to left, suppose $\neg X$ is a contradiction. Then, there is no $\mathbf{S}$ such that $\mathbf{S} \models \neg X$. So \textit{every} $\mathbf{S}$ is such that $\mathbf{S} \not\models \neg X$. But any $\mathbf{S} \not\models \neg X$ just in case $\mathbf{S} \models X$. So every $\mathbf{S} \models X$, and thus $X$ is a tautology. 
\end{proof}

\begin{remark}
Notice the form of this proof. Since we proving a biconditional (an \textit{iff} statement), we need to prove that the left hand side entails the right hand side, and the right hand side entails the left hand side. In order to prove that one side entails the other, we have to \textit{assume} that one side holds, and \textit{derive} that the other side then must also hold. Thus, we assumed the left hand side and derived the right, then assumed the right hand side, and derived the left. 
\end{remark}

As before, inverting this biconditional by negating both sides also gets us a true biconditional. Thus:

\begin{prop}
A formula $X$ of $\mathcal{L}_0$ is \emph{not} a tautology \textit{iff} its negation $\neg X$ is \emph{not} a contradiction. 
\end{prop}

Here is an additional proposition that may help here:

\begin{prop}
A formula $X$ of $\mathcal{L}_0$ is contingent \textit{iff} $\neg X$ is contingent. \label{contiff}
\end{prop}

\begin{proof}
From left to right, if $X$ is contingent, it is true in some structure $\mathbf{S}$, and false in some structure $\mathbf{S}'$. Now if $\mathbf{S} \models X$, then $\mathbf{S} \not\models \neg X$, so $\neg X$ is false in $\mathbf{S}$. On the other hand, if $\mathbf{S}' \not\models X$, then $\mathbf{S}' \models \neg X$, so $\neg X$ is true in $\mathbf{S}$. So $\neg X$ is also contingent. 

From right to left, the reasoning is almost exactly the same. 
\end{proof}

\begin{exc}
Finish the proof for Proposition \ref{contiff}. 
\end{exc}

Now given the above two propositions, we can see that if a formula $X$ of $\mathcal{L}_0$ is \textit{not} a tautology, then $\neg X$ is not a contradiction, because if $X$ is not a tautology, it is either a contradiction or contingent, so $\neg X$ will either be a tautology (hence not a contradiction), or contingent (hence not a contradiction). And similarly the other way around. 

In fact, the following also follows:

\begin{prop}
$\neg X$ is a tautology \textit{iff} $X$ is a contradiction. 
\end{prop}

\begin{proof}
By Proposition \ref{tautcon}, $Y$ is a tautology \textit{iff} $\neg Y$ is a contradiction. Here is the trick. Since $Y$ can be \textit{any formula}, we substitute $\neg X$ for $Y$. Thus, $\neg X$ is a tautology \textit{iff} $\neg \neg X$ is a contradiction. But $\neg \neg X$ is a contradiction \textit{iff} $X$ is also a contradiction. This is because for any $\mathbf{S}$, if $\mathbf{S} \models \neg \neg X$, then $\mathbf{S} \not\models \neg X$, then $\mathbf{S} \models X$ (and similarly the other way around). So $\neg X$ is a tautology \textit{iff} $X$ is a contradiction. Which is what we wanted to show. 
\end{proof}

Some of what the above propositions tell us is summarized in Table \ref{interrel}.

\begin{table}[h!]
\begin{center}
\begin{tabular}{lcl}
	& if, and only if & \\ \hline
	$X$ is tautology& $\Leftrightarrow$ & $\neg X$ is contradiction\\
	$X$ is contradiction& $\Leftrightarrow$ & $\neg X$ is tautology\\
	$X$ is contingent& $\Leftrightarrow$ & $\neg X$ is contingent
\end{tabular}
\end{center}
\label{interrel}
\caption{Relations between tautologies, contradictions and contingencies}
\end{table}

Here is another thing that will be relevant presently. Suppose you have a formula $X$, and it is \textit{not} a tautology (so it is either a contradiction or contingent). Then, its negation $\neg X$ will \textit{not} be a contradiction (it will either be a tautology or contingent). Thus, if $X$ is not a tautology, there is at least one structure $\mathbf{S}$ such that $\mathbf{S} \models \neg X$, and \textit{vice versa}. This is usually what is called a \textit{counterexample}. In other words:

\begin{prop}
$X$ is not a tautology \textit{iff} there is a structure $\mathbf{S}$ such that $\mathbf{S} \models \neg X$. 
\end{prop}

\begin{exc}
Using the reasoning with \textit{counterexamples} sketched above, show for each of the following formulas that they are \textit{not} tautologies. 

\begin{enumerate}
	\item $(\neg Y \wedge \neg X) \wedge \neg X$
	\item $(X \rightarrow Y) \rightarrow X$
	\item $(X \rightarrow Y) \rightarrow Y$
	\item $(X \vee Y) \rightarrow X$
\end{enumerate}
\end{exc}


\section{The basic idea behind tableau systems}

Here is the basic idea behind tableau systems. Imagine that your goal is to show that certain formulas of the language $\mathcal{L}_0$ are tautologies, or that in every structure $\mathbf{S}$, they are true. Now clearly, given a formula $X$, you cannot just consider every structure $\mathbf{S}$ individually to see whether that particular structure models $X$ or not. That would take an infinite amount of time. What you \textit{can} do is employ some of the proofs we have given above to show that, e.g., $X \vee \neg X$ is a tautology. The problem with this is that as the formulas get more complex, the proofs get longer and longer, to the point where it is just not feasible to carry them out in natural language. Moreover, these solutions are not very systematic as they stand. If you are ingenious, you may be able to do them (up to a point), but we don't want to rely on ingenuity to solve these problems. We need a method.  

In the tableau method, the whole approach is based on the following facts, discussed above:

\begin{enumerate}
	\item if $X$ is a tautology, $\neg X$ is a contradiction;
	\item if something is a contradiction, then it hides an explicit contradictory claim of form $Y \wedge \neg Y$ (and indeed, for some atomic formula $P$, $P \wedge \neg P$).
\end{enumerate}

If we put together these two facts, we get a strategy. We take the candidate tautology $X$. If it \textit{really} is a tautology, then its negation $\neg X$ is a contradiction. If it is a contradiction, then with the help of some appropriate rules, we can hopefully show in what way it is a contradictory statement. In other words, we show which is the formula $Y$ it claims is both true and false. 

On the other hand, if $X$ is not a tautology, then it is either a contradiction or contingent, so its negation $\neg X$ will either be a tautology or contingent (as described above). In such cases, we will find no contradiction. 

To continue our example, take $X \vee \neg X$ again. We already know this is a tautology, but what if we want to show in our tableau system that it is? Well, since we want to show $X \vee \neg X$, what we put at the beginning of our proof is \textit{its negation} $\neg (X \vee \neg X)$. Then come the rules, which will tease out where exactly is the contradictory statement hidden in the formula. 


\section{The rules of the system}

Each rule in our tableau system codifies some simple facts about our semantics. This is a general feature of deductive systems in logic. What is less so is the fact that tableau systems are not linear, but take a tree form. In fact, our tableau deductions will look something like the syntax trees we covered before. However, despite the similarities, do not confuse the two. They are entirely different systems with a different purpose. Tableau tree deductions are used to show whether $X$ is a \textit{tautologous} formula, syntax tree derivations are used to show whether $X$ is a formula at all.  

Just as syntax trees, tableau trees are also \textit{binary}. This means that at any one point, the tree may branch to at most two separate points. But sometimes, there is no branching, and only one additional point is connected. Because of this, we may classify our rules as those which branch, and those which do not. 

The basic idea behind the rules is that given a complex formula $X$ of the language assumed to be true, we can infer what other, less complex formulas must also be true as a consequence. For this, we need to know what formulas we may encounter in our language, and what we can do with them. 

First, we have the basic case, where everything is either an atomic formula $P$, a conjunction $Y \wedge Z$, a disjunction $Y \vee Z$, a conditional $Y \rightarrow Z$, or a negation $\neg Y$. Now as it turns out, it is a good idea to take the negation of each of these formulas separately. So we have the negation of an atomic formula $\neg P$, the negation of a conjunction $\neg (Y \wedge Z)$, the negation of a disjunction $\neg(Y \vee Z)$, the negation of a conditional $\neg (Y \rightarrow Z)$, the negation of a negation $\neg \neg Y$, or the negation of an atomic formula $\neg P$. This covers every possible negated formula form, and the two sets cover every possible formula form. Here is a diagram of this:

\begin{center}
\begin{forest}
	[Formula of $\mathcal{L}_0$
		[Atomic: $P$]
		[Complex: $X$
			[Non-negated
				[$Y \wedge Z$]
				[$Y \vee Z$]
				[$Y \rightarrow Z$]
			]
			[Negated
				[$\neg (Y \wedge Z)$]
				[$\neg (Y \vee Z)$]
				[$\neg (Y \rightarrow Z)$]
				[$\neg \neg Y$]
				[$\neg P$]
			]
		]
	]
\end{forest}
\end{center}

\begin{remark}
	It might seem like there is a problem here, since $\neg Y$ does not appear in the diagram, only $\neg \neg Y$. But this is not the case, since if we consider $\neg Y$, and what $Y$ could be, it could either be a negated formula or a non-negated one. If it is negated, then it is a double negation, which is covered by $\neg \neg Y$. If it is a simple negation, then $Y$ is either a complex or an atomic formula, which is also covered by the diagram. So there is nothing missing. 
\end{remark}

Now, as far as our rules are concerned, we will not have rules for $P$ and $\neg P$, but we will have rules for every other form. The principles behind these rules will be familiar if you have ever taken a logic course before. 

\subsection{Non-branching rules}

First, we will consider non-branching rules, since they are simpler. And we shall start with the simplest of them all. 

\subsubsection{Double negation}

The double negation rule is extremely simple. If you have a formula of form $\neg \neg X$ occurring on a branch of the tree, you can extend that branch with $X$. In practice, this simply looks like this:

\begin{figure}[h]
\centering
\begin{prooftree}{}
	[{\neg\neg X}
	[{X}, just=$\neg\neg$:!u]
	]
\end{prooftree}
\caption{Double negation rule}
\end{figure}

Why does this rule work? Simply because of the way structures are defined. In particular, if you have a structure $\mathbf{S}$, and $\mathbf{S} \models \neg \neg X$, then by definition of $\neg$, $\mathbf{S} \not\models \neg X$, and so by the same definition, $\mathbf{S} \models X$. This works the other way around too, but we are always moving from more complex to simpler formulas in tableau systems, so we do not need to consider the reverse. 

Here is a representation of the above in a truth table:

\begin{center}
	\begin{tabular}{c|c|c}
		$X$ & $\neg X$ & $\neg \neg X$\\ \hline \rowcolor{lightgray}
		$\mathbf{T}$ & $F$ & $\mathbf{T}$\\
		$F$ & $T$ & $F$
	\end{tabular}
\end{center}

\subsubsection{Conjunction}

The conjunction rule is another one of these really simple rules. It says that if you have a formula of form $X \wedge Y$ on a branch, then you can extend that branch with either $X$ or $Y$, i.e., with either the left or the right side. Like this:

\begin{figure}[h]
	\begin{minipage}{0.5\textwidth}\centering
		\begin{prooftree}{}
			[{X\wedge Y}
			[{X}, just=$\wedge$:!u]
			]
		\end{prooftree}
	\end{minipage}
	\begin{minipage}{0.5\textwidth}\centering
		\begin{prooftree}{}
			[{X\wedge Y}
			[{Y}, just=$\wedge$:!u]
			]
		\end{prooftree}
	\end{minipage}
	\caption{Conjunction rule}
\end{figure}

Clearly, by two successive applications of the rule on $X \wedge Y$, you can also get the following:

\begin{center}
\begin{prooftree}{}
	[{X\wedge Y}
		[{X}, just=$\wedge$:!u
			[{Y}, just=$\wedge$:!uu]
		]
	]
\end{prooftree}
\end{center}

Now why does this rule work? Again, it is a simple matter of checking that whenever $\mathbf{S} \models X \wedge Y$, $\mathbf{S} \models X$, and $\mathbf{S} \models Y$. In fact, this is literally just the definition of how $\wedge$ behaves in any structure, so we don't have to show anything. Of course, we can represent this in a truth table, the exact one that was given above for $\wedge$:

\begin{center}
\begin{tabular}{c|c|c} 
	$X$ & $Y$ & $X \wedge Y$\\ \hline \rowcolor{lightgray}
	$\mathbf{T}$ & $\mathbf{T}$ & $\mathbf{T}$\\
	$T$ & $F$ & $F$\\
	$F$ & $T$ & $F$\\
	$F$ & $F$ & $F$
\end{tabular}
\end{center}

\subsubsection{Negated disjunction}

The next type of non-branching rule that we cover is a bit more elaborate than the previous ones. It is of the following form, similar to the simple $\wedge$ rule above. 

\begin{figure}[h]
	\begin{minipage}{0.5\textwidth}\centering
		\begin{prooftree}{}
			[{\neg (X\vee Y)}
			[{\neg X}, just=$\neg\vee$:!u]
			]
		\end{prooftree}
	\end{minipage}
	\begin{minipage}{0.5\textwidth}\centering
		\begin{prooftree}{}
			[{\neg(X\vee Y)}
			[{\neg Y}, just=$\neg\vee$:!u]
			]
		\end{prooftree}
	\end{minipage}
	\caption{Negated disjunction rule}
\end{figure}

Now why does \textit{this} work? The above is an instance of what may be called DeMorgan's laws. The relevant instance is the following (usually formulated in the second part as $\neg X \wedge \neg Y$): 

\begin{prop}
If $\mathbf{S} \models \neg (X \vee Y)$, then $\mathbf{S} \models \neg X$ and $\mathbf{S} \models \neg Y$. 
\end{prop}

\begin{proof}
Suppose $\mathbf{S} \models \neg (X \vee Y)$. Then, $\mathbf{S} \not\models X \vee Y$. Now $\mathbf{S} \models X \vee Y$ just in case either $\mathbf{S} \models X$, $\mathbf{S} \models Y$, or both. Thus if $\mathbf{S} \not\models X \vee Y$, that means none of these cases hold. So that leaves us with $\mathbf{S} \not\models X$ and $\mathbf{S} \not\models Y$. Thus, $\mathbf{S} \models \neg X$ and $\mathbf{S} \models \neg Y$. 
\end{proof}

As you can see, our rule just codifies the above instance of DeMorgan's law. And once again, with two successive application of the rule, you can get the negation of either formula $X$ or formula $Y$, if needed. Compare the following table:

\begin{center}
\begin{tabular}{c|c|c|c|c|c}
$X$ & $Y$ & $\neg X$ & $\neg Y$ & $X \wedge Y$ & $\neg (X \wedge Y)$\\ \hline
$T$ & $T$ & $F$ & $F$&$T$& $F$\\
$T$ & $F$ & $F$ & $T$& $F$& $F$\\
$F$ & $T$ & $T$ & $F$ & $F$ & $F$\\ \rowcolor{lightgray}
$F$ & $F$ & $\mathbf{T}$ & $\mathbf{T}$& $F$& $\mathbf{T}$
\end{tabular}
\end{center}

\medskip
\hrule
\medskip

We can stop now for a second and look at our initial question, that of proving that $X \vee \neg X$ is a tautology. As mentioned, in order to do this, we need to put its \textit{negation} at the top of our tree. Then, we have to apply our rules until we get to an explicit contradiction of form $Y \wedge \neg Y$ (for some $Y$). Thus:

\begin{center}
\begin{prooftree}{}
	[{\neg (X \vee \neg X)}, just=Start
		[{\neg X}, just=$\neg\vee$:!u
		[{\neg \neg X}, just=$\neg\vee$:!uu
		[{X}, just=$\neg \neg$:!u, close={:!uu, !c}]
		]
		]
	]
\end{prooftree}
\end{center}

The above is a complete proof of $X \vee \neg X$ being a tautology. You might be asking what the symbol $\otimes$ stands for at the end. It represents that the branch (the only one of the tree) is `closed', because both a formula and its negation occurs on it. Where? Well, that is given by the numbers below $\otimes$, namely line 2 and 4. Line 2 has the formula $\neg X$, while line 4 has the formula $X$ on it, so we have both $X$ and $\neg X$. Thus, we have our desired explicit contradiction derived from the negation of the tautology candidate. This means that $X \vee \neg X$ \textit{is} a tautology according to our system. Nice!

\medskip\hrule\medskip

Let's get back to our other rules before we go further into examples. 

\subsubsection{Negated conditional}

The negated conditional is another one of those non-branching rules that are not immediately obvious. It is specified as follows: 

\begin{figure}[h]
	\begin{minipage}{0.5\textwidth}\centering
		\begin{prooftree}{}
			[{\neg (X\rightarrow Y)}
			[{X}, just=$\neg\rightarrow$:!u]
			]
		\end{prooftree}
	\end{minipage}
	\begin{minipage}{0.5\textwidth}\centering
		\begin{prooftree}{}
			[{\neg(X\rightarrow Y)}
			[{\neg Y}, just=$\neg\rightarrow$:!u]
			]
		\end{prooftree}
	\end{minipage}
	\caption{Negated conditional rule}
\end{figure}

Why does this work? Again, it just codifies a fact, the following fact, from our semantics:

\begin{prop}
	If $\mathbf{S} \models \neg (X \rightarrow Y)$, then $\mathbf{S} \models X$ and $\mathbf{S} \models \neg Y$. 
\end{prop}

\begin{proof}
Suppose $\mathbf{S} \models \neg (X \rightarrow Y)$. Then, $\mathbf{S} \not\models X \rightarrow Y$. But $\mathbf{S} \models X \rightarrow Y$ \textit{iff} whenever $\mathbf{S} \models X$, $\mathbf{S} \models Y$. So again, by negating both sides of the biconditional, $\mathbf{S} \not\models X \rightarrow Y$ \textit{iff} it is not the case that if $\mathbf{S} \models X$, then $\mathbf{S} \models Y$. So $\mathbf{S} \not\models X \rightarrow Y$ \textit{iff} $\mathbf{S} \models X$ but $\mathbf{S} \not\models Y$. Or in other words, \textit{iff} $\mathbf{S} \models X$ and $\mathbf{S} \models \neg Y$. Which is what we wanted to show. 
\end{proof}

Here is the truth-table fragment to compare:

\begin{center}
	\begin{tabular}{c|c|c|c|c}
	$X$ & $Y$ & $\neg Y$ & $X \rightarrow Y$ & $\neg (X \rightarrow Y)$\\ \hline
	$T$ & $T$ & $F$& $T$& $F$ \\\rowcolor{lightgray}
	$\mathbf{T}$ & $F$ & $\mathbf{T}$& $F$& $\mathbf{T}$ \\
	$F$ & $T$ & $F$& $T$& $F$\\
	$F$ & $F$ & $T$ & $T$ & $F$
	\end{tabular}
\end{center}

This concludes our non-branching rules. Now we turn to our branching ones. 

\subsection{Branching rules}

As mentioned above, our tree only ever branches to two separate branches from any one point. Thus, our branching rules are all of the form:

\begin{center}
\begin{forest}
	[$X$
		[$Y$]
		[$Z$]
	]
\end{forest}
\end{center}

Why do we need branching rules? Note that in all of the above examples, the assumption that a certain complex formula is true determined exactly the truth values of some simpler formulas. In the truth table illustrations, this just means that for each complex formula, they were only true in one, and only one, row (the one in gray), so we just had to check what truth values the simpler formulas received in that row (put in bold). However, many formulas are true in several rows in a truth-table. In that case, we need to consider the possibilities that afford the complex formula to be true separately. And this gets compounded if we go further and encounter another such formula, and another one, and so on. 

\subsubsection{Disjunction}

The disjunction rule is the simplest rule of the branching ones. In many ways, it is like the conjunction rule. It looks like this:

\begin{figure}[h]
	\centering
	\begin{prooftree}{}
		[{X\vee Y}
		[{X}, just=$\vee$:!u]
		[{Y}]
		]
	\end{prooftree}
	\caption{Disjunction rule}
\end{figure}

Here is the reasoning behind this rule. As we know, $\mathbf{S} \models X \vee Y$ \textit{iff} $\mathbf{S} \models X$ or $\mathbf{S} \models Y$ (or both). The problem is that just the fact that $\mathbf{S} \models X \vee Y$ does not tell us whether $\mathbf{S} \models X$ or $\mathbf{S} \models Y$ or both. It only tells us that \textit{it is at least one of these}. Given this lack of information, we need to consider the consequences separately. Interestingly, we need not consider the case where \textit{both} $X$ and $Y$ is true separately. But we also don't make any explicit assumptions about what the truth-value of the \textit{other} formula is on either branch. So all we do is consider the case where $X$ is true, whatever truth value $Y$ may have ($T$ or $F$), and consider the case where $Y$ is true, whatever truth value $X$ may have ($T$ or $F$). Here is the table representation:

\begin{center}
	\begin{tabular}{c|c|c}
		$X$ & $Y$ & $X \vee Y$\\ \hline \rowcolor{lightgray}
		$\mathbf{T}$ & $\mathbf{T}$ & $\mathbf{T}$\\ \rowcolor{lightgray}
		$\mathbf{T}$ & $F$ & $\mathbf{T}$\\ \rowcolor{lightgray}
		$F$ & $\mathbf{T}$ & $\mathbf{T}$\\
		$F$ & $F$ & $F$
	\end{tabular}
\end{center}

As described above, when we branch to $X$, we are considering \textit{either} line 1 or line 2 of the truth table. When we branch to $Y$, we are considering \textit{either} line 1 or line 3 of the truth table. 


\subsubsection{Negated conjunction}

Negated conjunction is also an instance of DeMorgan's laws. It is as follows:

\begin{figure}[h]
	\centering
	\begin{prooftree}{}
		[{\neg (X\wedge Y)}
		[{\neg X}, just=$\neg\wedge$:!u]
		[{\neg Y}]
		]
	\end{prooftree}
	\caption{Negated conjunction rule}
\end{figure}



Again, we can show the following:

\begin{prop}
If $\mathbf{S} \models \neg (X \wedge Y)$, then $\mathbf{S} \models \neg X$ or $\mathbf{S} \models \neg Y$ (or both). 
\end{prop}

\begin{proof}
Suppose $\mathbf{S} \models \neg (X \wedge Y)$. Then, $\mathbf{S} \not\models X \wedge Y$. But $\mathbf{S} \models X \wedge Y$ \textit{iff} $\mathbf{S} \models X$ and $\mathbf{S} \models Y$, so $\mathbf{S} \not\models X \wedge Y$ \textit{iff} either $\mathbf{S} \not\models X$ or $\mathbf{S} \not\models Y$, or both. So $\mathbf{S} \models \neg X$, or $\mathbf{S} \models \neg Y$, or both. 
\end{proof}

Again, on either branch, we do not make assumptions about the truth value of the formula on the other branch. Thus, on the left branch, we assume $\neg X$ is true, but nothing about $\neg Y$ being true or false, and conversely for the right branch. In table representation:

\begin{center}
	\begin{tabular}{c|c|c|c|c|c}
		$X$ & $Y$ & $\neg X$ & $\neg Y$ & $X \wedge Y$ & $\neg (X \wedge Y)$\\ \hline
		$T$ & $T$ & $F$ & $F$ & $T$ & $F$ \\ \rowcolor{lightgray}
		$T$ & $F$ & $F$ & $\mathbf{T}$ & $F$ & $\mathbf{T}$ \\ \rowcolor{lightgray}
		$F$ & $T$ & $\mathbf{T}$& $F$ & $F$ & $\mathbf{T}$ \\ \rowcolor{lightgray}
		$F$ & $F$ & $\mathbf{T}$ & $\mathbf{T}$ & $F$ & $\mathbf{T}$
	\end{tabular}
\end{center}

\clearpage
\subsubsection{Conditional}

The rule for conditionals is our last rule for the system. It also has a form that may make you stop and think why it is formulated as such. 

\begin{figure}[h]
	\centering
	\begin{prooftree}{}
		[{X \rightarrow Y}
		[{\neg X}, just=$\rightarrow$:!u]
		[{Y}]
		]
	\end{prooftree}
	\caption{Conditional rule}
\end{figure}

The reasoning behind it is as follows:

\begin{prop}
If $\mathbf{S} \models X \rightarrow Y$, then $\mathbf{S} \models \neg X$ or $\mathbf{S} \models Y$ (or both). 
\end{prop}

\begin{proof}
The proof follows the usual lines. Suppose $\mathbf{S} \models X \rightarrow Y$. Then, whenever $\mathbf{S} \models X$, $\mathbf{S} \models Y$. We have already seen that this is \textit{false} when $\mathbf{S} \models X$ but $\mathbf{S} \not\models Y$. This leaves 3 other options left for it to be \textit{true}.

\begin{enumerate}
	\item First, if $\mathbf{S} \models X$ and $\mathbf{S}\models Y$, clearly, $\mathbf{S} \models X \rightarrow Y$. Note that $\mathbf{S} \models X$ only if $\mathbf{S} \not\models \neg X$, but $\mathbf{S} \models Y$.
	\item Second, if $\mathbf{S} \not\models X$ but $\mathbf{S} \models Y$, then $\mathbf{S}\models X \rightarrow Y$ again. This is the case where both $\mathbf{S} \models \neg X$ (because $\mathbf{S} \not\models X$) \textit{and} $\mathbf{S} \models Y$. 
	\item Third, if $\mathbf{S} \not\models X$ and $\mathbf{S} \not\models Y$, then still, $\mathbf{S} \models X \rightarrow Y$. Again, this means that $\mathbf{S} \models \neg X$, but $\mathbf{S} \not\models Y$ in this case. 
\end{enumerate} 

As you can see, in all three cases where $\mathbf{S} \models X \rightarrow Y$, it is true that either $\mathbf{S} \models \neg X$ or $\mathbf{S} \models Y$ or both. Indeed, those are exactly the three options we have for $X \rightarrow Y$ to be true in $\mathbf{S}$. 
\end{proof}

Or in truth table form:

\begin{center}
\begin{tabular}{c|c|c|c} 
	$X$ & $Y$ & $\neg X$ & $X \rightarrow Y$\\ \hline \rowcolor{lightgray}
	$T$ & $\mathbf{T}$ & $\mathbf{F}$ & $\mathbf{T}$\\
	$T$ & $F$ & $F$ & $F$\\\rowcolor{lightgray}
	$F$ & $\mathbf{T}$ & $\mathbf{T}$ & $\mathbf{T}$\\\rowcolor{lightgray}
	$F$ & $\mathbf{F}$ & $\mathbf{T}$ & $\mathbf{T}$
\end{tabular}
\end{center}

As usual, the rule form only explicitly represents the possibility where $\neg X$ is true, and the possibility where $Y$ is true, leaving open whether $Y$ is true or false on the left branch (and similarly for $\neg X$ on the right one). 

\subsubsection{The strategy}

Here is the strategy again, now with all our rules in hand. Suppose you have a formula $X$ that is a candidate for being a tautology. In order for us to show this in our system, we first have to take $\neg X$, and put that at the \textit{root} of our tree. Then, we have to apply our rules until we have an explicit contradiction of form $Y$ and $\neg Y$ on \textit{each} branch of the tree. 

Suppose we want to show something quite elaborate, like the following: 
\[(X \rightarrow Y) \rightarrow (\neg X \vee Y)\]
\begin{remark}
You may note the similarity between this \textit{formula schema} and the \textit{rule} for conditionals. Of course, this is no accident. In fact, every one of our rules can be turned into a tautology in our system, with the use of $\rightarrow$.  
\end{remark}

Since the formula is $(X \rightarrow Y) \rightarrow (\neg X \vee Y)$, again, we put its negation at the root, thus:

\begin{center}
\begin{prooftree}{}
	[{\neg((X \rightarrow Y) \rightarrow (\neg X \vee Y))}, just=Start]
\end{prooftree}
\end{center}

The next step is to apply one of the rules. For this, you need to identify whether the formula is negated or not, and what the main connective of the formula is, or in the negated case, what the main connective \textit{after} the negation sign is. Here, the answer is: it is a negated conditional. Obviously, you then apply the negated conditional rule. In fact, we don't quite see where we are going yet, so we can apply it twice to get both possibilities. Thus:

\begin{center}
	\begin{prooftree}{}
		[{\neg((X \rightarrow Y) \rightarrow (\neg X \vee Y))}, just=Start
		[{X \rightarrow Y}, just=$\neg\rightarrow$:!u
		[{\neg(\neg X \vee Y)}, just=$\neg\rightarrow$:!uu]
		]
		]
	\end{prooftree}
\end{center}

Now that we have two formulas, we have a choice how we want to proceed. On the other hand, unlike with other systems, we do not have a choice about which rules to apply. Each formula admits of one, and only one, rule, depending on its form. 

In general, it is usually better to apply first all the rules that do not branch, and \textit{then} apply any branching rules. If you think about it, this makes sense, since each branching rule doubles the work we have to carry out. At any rate, the non-branching rule is the one for $\neg \vee$, so we can start with that, and apply it twice: 

\begin{center}
	\begin{prooftree}{}
		[{\neg((X \rightarrow Y) \rightarrow (\neg X \vee Y))}, just=Start
		[{X \rightarrow Y}, just=$\neg\rightarrow$:!u
		[{\neg(\neg X \vee Y)}, just=$\neg\rightarrow$:!uu
		[{\neg\neg X}, just=$\neg\vee$:!u
		[{\neg Y}, just=$\neg\vee$:!uu]
		]
		]
		]
		]
	\end{prooftree}
\end{center}

So far, we only have a single branch, and that branch is open. On the other hand, we still have formulas on which to apply rules. So we are certainly not done. One trivial step is to get rid of the double negation in line 4. This will not help us much, but it will simplify things a bit. Thus:

\begin{center}
	\begin{prooftree}{}
		[{\neg((X \rightarrow Y) \rightarrow (\neg X \vee Y))}, just=Start
		[{X \rightarrow Y}, just=$\neg\rightarrow$:!u
		[{\neg(\neg X \vee Y)}, just=$\neg\rightarrow$:!uu
		[{\neg\neg X}, just=$\neg\vee$:!u
		[{\neg Y}, just=$\neg\vee$:!uu
		[{X}, just=$\neg\neg$:!uu]
		]
		]
		]
		]
		]
	\end{prooftree}
\end{center}

Now most of the formulas in the proof are `exhausted'. There is no rule for $\neg Y$ or $X$ to apply, and for all the other formulas (except one), we have applied the appropriate rules as many times as possible. So we are left with $X \rightarrow Y$. This is a branching rule, so now we branch. 

\begin{center}
	\begin{prooftree}{}
	[{\neg((X \rightarrow Y) \rightarrow (\neg X \vee Y))}, just=Start
	[{X \rightarrow Y}, just=$\neg\rightarrow$:!u
	[{\neg(\neg X \vee Y)}, just=$\neg\rightarrow$:!uu
	[{\neg\neg X}, just=$\neg\vee$:!u
	[{\neg Y}, just=$\neg\vee$:!uu
	[{X}, just=$\neg\neg$:!uu
		[{\neg X}, just=$\rightarrow$:!uuuuu]
		[{Y}, just=$\rightarrow$:!uuuuu]
	]
	]
	]
	]
	]
	]
	\end{prooftree}
\end{center}

Our tree is now complete, there are no more rules to apply. Now comes the crucial part. You have to see if each of the branches are \textit{closed}. Again, this means that you have a formula of form $Z$ and a formula of form $\neg Z$ on each branch. Note that each branch is the sequence of formulas from the root to one of the leaves. Thus, the first branch here is from $\neg((X \rightarrow Y) \rightarrow (\neg X \vee Y))$ to $\neg X$, the second branch is from $\neg((X \rightarrow Y) \rightarrow (\neg X \vee Y))$ to $Y$, through all the other formulas. 

Let's take the first branch. Can you see a contradiction? I can, since line 6 has $X$ on it, and line 7 (left) has $\neg X$ on it. So the first branch is closed. Thus: 

\begin{center}
	\begin{prooftree}{}
		[{\neg((X \rightarrow Y) \rightarrow (\neg X \vee Y))}, just=Start
		[{X \rightarrow Y}, just=$\neg\rightarrow$:!u
		[{\neg(\neg X \vee Y)}, just=$\neg\rightarrow$:!uu
		[{\neg\neg X}, just=$\neg\vee$:!u
		[{\neg Y}, just=$\neg\vee$:!uu
		[{X}, just=$\neg\neg$:!uu
		[{\neg X}, just=$\rightarrow$:!uuuuu, close={:!u,!c}]
		[{Y}, just=$\rightarrow$:!uuuuu]
		]
		]
		]
		]
		]
		]
	\end{prooftree}
\end{center}

What about the second branch? Again, there is a contradiction, since line 5 has $\neg Y$, while line 7 (right) has $Y$. Thus: 

\begin{center}
	\begin{prooftree}{}
		[{\neg((X \rightarrow Y) \rightarrow (\neg X \vee Y))}, just=Start
		[{X \rightarrow Y}, just=$\neg\rightarrow$:!u
		[{\neg(\neg X \vee Y)}, just=$\neg\rightarrow$:!uu
		[{\neg\neg X}, just=$\neg\vee$:!u
		[{\neg Y}, just=$\neg\vee$:!uu
		[{X}, just=$\neg\neg$:!uu
		[{\neg X}, just=$\rightarrow$:!uuuuu, close={:!u,!c}]
		[{Y}, just=$\rightarrow$:!uuuuu,, close={:!uu,!c}]
		]
		]
		]
		]
		]
		]
	\end{prooftree}
\end{center}

This completes the proof. Once you have a complete proof of a formula in the system, you can say that that formula is a \textit{theorem} of zeroth-order logic. Thus, $(X \rightarrow Y) \rightarrow (\neg X \vee Y)$ is a theorem of zeroth order logic. We represent this as follows: 
\[
\vdash(X \rightarrow Y) \rightarrow (\neg X \vee Y)
\]
The symbol $\vdash$ means something like `\textit{syntactically provable}'. It is the syntactic equivalent to being a tautology in the semantics. In fact, being a tautology (in the semantics) can be represented as follows:
\[
\models(X \rightarrow Y) \rightarrow (\neg X \vee Y)
\]
This makes sense, as it is just like writing $\mathbf{S} \models (X \rightarrow Y) \rightarrow (\neg X \vee Y)$, except we omit a specific structure designation $\mathbf{S}$, since it is modeled by \textit{all} structures, not just a specific one. 

\begin{defn}[Theorem]
Let $X$, $Y$ be any formulas. We call a branch of a tree \textit{closed} provided there are some formulas of form $Y$, $\neg Y$ occurring on it. We call a tree \textit{closed} if all its branches are closed. Then, $X$ is a \textit{theorem of zeroth-order logic} provided there is a closed tree with $\neg X$ at its root. In such cases, we also write $\vdash X$, and call the closed tree for $\neg X$ the tableau (tree) proof of $X$. 
\end{defn}

\medskip\hrule\medskip

Here is another proof to inspect, with more branching, before it is your turn:

\begin{center}
	\begin{prooftree}{}
[{\neg (((Y \vee \neg Y)\rightarrow X) \rightarrow (X \wedge (Y \vee \neg Y)))}, just=Start
		[{(Y \vee \neg Y) \rightarrow X}, just=$\neg\rightarrow$:!u
		[{\neg (X \wedge (Y \vee \neg Y))}, just=$\neg\rightarrow$:!uu
		[{\neg(Y \vee \neg Y)}, just=$\rightarrow$:!uu
		[{\neg Y}, just=$\neg\vee$:!u
		[{\neg\neg Y}, just=$\neg\vee$:!uu, close={:!u, !c}]
		]
		]
		[{X}
		[{\neg X}, just=$\neg\wedge$:!uu, close={:!u, !c}]
		[{\neg (Y \vee \neg Y)}
		[{\neg Y}, just=$\neg\vee$:!u
		[{\neg\neg Y}, just=$\neg\vee$:!uu, close={:!u, !c}]
		]
		]
		]
		]
		]
		]
	\end{prooftree}
\end{center}

Note that we could have made this derivation even longer, but we opted to apply $\neg\wedge$ to line 3 to extend only the right-side branch of the tree (to line 5). This is because it would not have made any difference on the left-hand side, given that it closed by $\neg (Y \vee \neg Y)$ alone (line 4, then 8 and 9). In any derivation, you are looking to close every branch and nothing more. Thus, you can make derivations significantly shorter if you cut out the steps that are not necessary to close a branch. 

\fpboxstar{Here is an intuitive explanation of why $((Y \vee \neg Y)\rightarrow X) \rightarrow (X \wedge (Y \vee \neg Y))$ is a tautology. If you look at the first half of the formula, it says that $(Y \vee \neg Y) \rightarrow X$. I.e., if $Y \vee \neg Y$, then $X$. But since $Y \vee \neg Y$ is known to be a tautology that holds in all structures, \textit{if} it entails $X$, then $X$ would also be a tautology (since again, $Y \vee \neg Y$ holds in every structure, and so if in every structure, it entails $X$, $X$ holds in every structure). But \textit{if} that is the case, then both $Y \vee \neg Y$ and $X$ must hold (both being tautologies).}

\begin{exc}
	Prove that the following formulas are all theorems of the tableau system: 
	
	\begin{enumerate}
		\item $Y \rightarrow (X \rightarrow Y)$;
		\item $(\neg X \wedge \neg Y) \rightarrow \neg(X \vee Y)$;
		\item $(\neg X \vee \neg Y) \rightarrow \neg(X \wedge Y)$;
		\item $\neg (X \wedge Y) \rightarrow (\neg X \vee \neg Y)$;
		\item $\neg (X \vee Y) \rightarrow (\neg X \wedge \neg Y)$;
		\item $(\neg X \vee Y) \rightarrow (X \rightarrow Y)$;
		\item $(X \rightarrow Y) \rightarrow (\neg Y \rightarrow \neg X)$;
		\item $((X \vee Y) \wedge ((X \rightarrow Z) \wedge (Y \rightarrow Z))) \rightarrow Z$.
	\end{enumerate}
\end{exc}


\section{Some facts about the tableau system}

\subsection{On soundness and completeness}

A fundamental goal for every logical system is to show that the semantics and the syntactic system agree on every formula. That is, if $X$ is a tautology according to the semantics, it can be proved in the system that $X$ is a theorem, and if $X$ is a theorem in the system, it really is a tautology in the semantics. This can be represented as follows: 
\[\models X \textit{ iff } \vdash X\]

Or again, $X$ is a theorem \textit{iff} it is a tautology. I have been using these two terms somewhat interchangeably above, since as it stands, in the system I just introduced, they really do come down to the same thing. Usually, this is characterized two ways:

\begin{enumerate}
	\item if whenever $\mathbf{S} \vdash X$, then $\mathbf{S} \models X$, the system relative to the semantics is \textit{sound};
	\item if whenever $\mathbf{S} \models X$, then $\mathbf{S} \vdash X$, the system relative to the semantics is \textit{complete}. 
\end{enumerate}

Of course, if you put together these two conditionals, you get the biconditional above. In general, it is important to keep in mind that our semantics and our deductive system are two distinct things, and that they coincide on their judgments using two distinct methods is not a trivial feat (in fact, large classes of logics \textit{cannot} have a non-trivial sound and complete deductive system). On the other hand, \textit{proving} soundness and completeness is rather involved and takes more machinery than we currently possess (or will possess by the end of this book). Thus, for now, you will have to trust the process. 

The process, our tableau method, is in many ways better than other approaches you may have seen before, like natural deduction. The reason is because the tableau method does not require you to be creative in your proofs at all. In fact, it is provable that given a formula $X$, if $X$ is a tautology, then applying the relevant rule as many times as possible to $ \neg X$ and each resulting formula completely mindlessly will result in a tree all of whose branches \textit{will} close, and if $X$ is not a tautology, there will be at least one open branch left in the end. So in general, all you have to do is recognize for each formula which rules can be applied, and apply them. Of course, if you are an expert in using the system, you will be able to do shorter proofs by using your brain (e.g., by using non-branching rules only once, deriving the relevant side only). 

\subsection{Open tableaus and their uses}

In fact, here is another way in which tableaus are better than natural deduction. Consider again the above crucial fact about our syntax and deductive system:

\[\models X \textit{ iff } \vdash X\]

Now using the usual technique on the biconditional, negate both sides. Thus: 

\[\not\models X \textit{ iff } \nvdash X\]

Now we can think about what \textit{this} means. First, $\not\models X$ means that $X$ is not a tautology. Second, $\nvdash X$ means that $X$ is not a theorem of the system. The latter in turn just means that $\neg X$ (!)  does not (cannot) have a closed tableau. So it can only have open tableaus. So $X$ is not a tautology provided $\neg X$ only has open tableaus. So far so good. Let's continue. 

What does it mean for $X$ to not be a tautology? Well, by definition, it means that $X$ is either a contradiction or contingent. And in turn, if $X$ is either a contradiction or contingent, then $\neg X$ is either a tautology or contingent. So injecting this back to our previous line of reasoning, we get:

\begin{prop}
$\neg X$ is either a tautology or contingent \textit{iff} $\neg X$ does not have a closed tableau. 
\end{prop} 

We can chase these facts a bit further. In particular, if $\neg X$ is either a tautology or contingent, that means that in either case, there is at least one structure $\mathbf{S}$ such that $\mathbf{S} \models \neg X$. In other words, if $\neg X$ does not have a closed tableau, we can immediately answer that it has a model! (And of course, the same goes for $X$.)

\section{Extending our approach to sets of formulas}

Students of logic are usually taught at first about tautologies, contradictions, and contingent propositions. However, these are not the notions you will encounter in more advanced, and more modern, works. This is at least partly because these notions are usually used for individual formulas, while a lot of the times, we want to talk about sets of formulas instead. 

One thing that we can immediately extend to sets of formulas is contingency. Sometimes, it is not just one formula that has a model, but a whole set of them. Indeed, perhaps an infinitely large set of them! In these cases, we sometimes say that the set $S$ of formulas is (zeroth-order) \textit{satisfiable} or \textit{semantically consistent}. This can be captured in a precise manner as follows:

\begin{defn}[Satisfiability]
A set $S$ of formulas of $\mathcal{L}_0$ is (zeroth-order) \textit{satisfiable} or \textit{semantically consistent} \textit{iff} there is a structure $\mathbf{S}$ such that $\mathbf{S} \models X$ for each $X \in S$. We say $S$ is (zeroth-order) \textit{unsatisfiable} or \textit{semantically inconsistent} provided it is not semantically consistent. 
\end{defn}

\begin{remark}
Note that the above definition is \textit{not} the same as claiming that for each $X \in S$, there is a structure $\mathbf{S}$ such that $\mathbf{S} \models X$. This would not make \textit{the set} consistent, it would make the formulas contingent individually. For example, if $X$ is contingent, then $\neg X$ is also contingent, so both $X$ and $\neg X$ have a model. But clearly, $\set{X, \neg X}$ is not consistent, since there is no structure in which both of these could be true.  
\end{remark}

Here is a useful way to think about consistency and contingency:

\begin{prop}
The formula $X$ is contingent \textit{iff} the singleton set $\set{X}$ is semantically consistent. \label{contcons}
\end{prop}

\begin{exc}
Prove Proposition \ref{contcons}. Hint: show from left to right that if $X$ is contingent, $\set{X}$ is consistent, and from right to left that if $\set{X}$ is consistent, $X$ is contingent. To do this, use the definitions for these notions. 
\end{exc}

Here is an interesting fact about consistency and contradictions:

\begin{prop}
If $S=\set{X_1, ..., X_n}$, and some $X \in S$ is a contradiction, then $S$ cannot be semantically consistent. 
\end{prop}

\begin{proof}
In other words, a single contradiction can make a set of formulas inconsistent. This is easy to see since semantic consistency demands that there be at least one structure $\mathbf{S}$ in which each member of $S$ is true. Now if $X$ is a contradiction, there is no structure in which it is true, and $X \in S$ by assumption. 
\end{proof}

On the other hand, we have:

\begin{prop}
If $S$ is a set of tautologies, $S$ is semantically consistent. 
\end{prop}

\begin{exc}
The proof is left as an exercise. 
\end{exc}
 
\subsection{Validity}

Here comes something magical. So far, we talked about certain sets of formulas that are satisfiable, and certain sets of formulas that are unsatisfiable. And this boiled down to whether the set of formulas have a common model, or they do not. Now as it turns out, satisfiability and unsatisfiability are sufficient to define the central notion of logic: validity. 

This may not be apparent at first. Take the usual definition of validity:

\begin{quote}
(A1) An argument is valid \textit{iff} whenever the premises are true, the conclusion must also be. 
\end{quote}

It seems like this has absolutely nothing to do with whether sets of formulas are satisfiable. Not so fast!

What is an argument? We can define it as such:

\begin{defn}
An argument is a set of formulas, of which one is designated the conclusion, and the rest are designated the premises. 
\end{defn}

This could be formalized with more structure, using ordered sets and whatnot, but what I want to emphasize here is that arguments are just sets of formulas with some designated members. 

Let's return to validity. Validity says that if an argument is valid, then whenever its premises are true, its conclusion must also be true. You may ask yourself: is it possible for a \textit{valid} argument's premises to be \textit{true}, but its conclusion to be \textit{false}? It seems not, since that is exactly what validity excludes. It says that \textit{if} a valid argument's premises happen to be true, then whatever else is the case, the conclusion \textit{must} also be true. Thus, we can also characterize validity as follows:

\begin{quote}
(A2) An argument is valid \textit{iff} it is \textit{impossible} for its premises to be true, but its conclusion to be false. 
\end{quote}

Note the word `impossible' in italics. What does it mean that it is impossible for the premises to be true and the conclusion to be false? Well, it just means: there cannot be a structure $\mathbf{S}$ such that $\mathbf{S}$ models the premises, but $\mathbf{S}$ does not model the conclusion. Let's use some specific variables to make this more precise. 

Suppose you have an argument with (possibly infinite) premises $P=\set{X_1, X_2, ...}$, and conclusion $Y$. Then, by the above, if this argument is \textit{valid}, that means there is no structure $\mathbf{S}$ such that $\mathbf{S} \models X$ for all $X \in P$, but $\mathbf{S} \not\models Y$ (the conclusion). We also know that $\mathbf{S} \not\models Y$ \textit{iff} $\mathbf{S} \models \neg Y$, so we can say: if an argument is \textit{valid}, that means there is no structure $\mathbf{S}$ such that $\mathbf{S} \models X$ for all $X \in P$, and $\mathbf{S} \models \neg Y$. Again, this is just a formal way of saying it is impossible for the premises to be true but the conclusion to be false (or equivalently, it is impossible for the premises to be true, and the negation of the conclusion to also be true). 

But note that now we really are talking about unsatisfiability! For what we just described may simply be put: the set $\{X_1, X_2, ..., \neg Y\}$ is unsatisfiable. And this is, indeed, what validity boils down to. In particular, an argument is valid \textit{if, and only if} its premises, together with the negation of its conclusion, are unsatisfiable. And again, all this just means that it is impossible for a valid argument to have true premises and a false conclusion. Thus:

\begin{defn}[Validity]
Let $A=\{X_1, X_2, ...\}$ be any set of formulas and let $Y$ be any formula of $\mathcal{L}_0$. Then, the argument from \textit{premises} $A$ to the \textit{conclusion} $Y$ is \textit{valid} \textit{iff} the set $\set{X_1, X_2, ..., \neg Y}$ is not satisfiable. If an argument from $A$ to $Y$ is \textit{valid}, we write $A \models Y$, or say that $Y$ is a \textit{semantic consequence} of, or \textit{semantically entailed} by $A$.  
\end{defn}

\begin{remark}
Indeed, from this, we get an alternative definition of being a tautology for free. For note that the premise set $A$ was not required to be non-empty. So $A$ may be $\emptyset$. So by definition, $\emptyset \models Y$ \textit{iff} $\set{\neg Y}$ is unsatisfiable. And $\set{\neg Y}$ is unsatisfiable \textit{iff} $Y$ is a tautology. In such cases, we can just write $\models Y$, as we have been doing. 
\end{remark}

\subsection{Validity and tableau}

From the above, you may already see how reasoning with sets of formulas is implemented in our system. With single formulas, we had that if \textit{every} possible tree for $\neg X$ remained open, that must mean $\neg X$ is satisfiable (i.e., it is a tautology or contingent), which in turn ensures that $X$ is \textit{not} a tautology. On the other hand, if there is a possible tree for $\neg X$ that closes (closes on all its branches), then $X$ is indeed a tautology, since its negation is not satisfiable. Similarly, but with sets of formulas, if a tree for $\{X_1, X_2, ..., \neg Y\}$ closes at least once, then $\{X_1, X_2, ..., \neg Y\}$ is \textit{not} satisfiable, which in turn means that $\{X_1, X_2, ..\}$ entails $Y$. On the other hand, if \textit{no} tree for $\{X_1, X_2, ..., \neg Y\}$ closes, that means the set is satisfiable, and $\{X_1, X_2, ...\}$ does not entail $Y$. 

The only question is: how do we work with multiple formulas, when in the simpler case, we put the sole formula $\neg X$ at the root of the tree (when trying to prove $X$ is a tautology)? First, with arguments, we put the \textit{negation} of the \textit{conclusion} at the root of the tree. So if we are considering whether the set $S=\set{X_1, X_2, ...}$ of premises is such that $S \vdash Y$, we put $\neg Y$ at the root of the tree. 

The other change is more substantial. We introduce a very simple new rule into the system that enables us to inject any one of the premises at any part of our derivation. Thus: 

\begin{figure}[h]
	\centering
	\begin{prooftree}{}
		[{X}
		[{Y}, just=Pr.]
		]
	\end{prooftree}
	
	\medskip
	\textit{Provided $Y$ is one of the premises.}
	\caption{Premise rule}
\end{figure}

Note that $X$ and $Y$ here are completely independent of one another. $X$ may be any formula that you happen to consider at any one point of your proof, and $Y$ can be any formula as long as it is one of the premises under consideration. Accordingly, the premise rule is radically different from all the previous rules, since it does not `depend' on any formula at any one point of the tree. In other words, you can put any premise down at any one point of your derivation, and in fact, as many times as you want. 

\fpboxstar{The relationship between the premise rule and all the other rules of our tableau system is similar to the relationship between the base rule and the productive rule(s) in our syntactic derivations.}

Suppose you want to show that $\{X, X \rightarrow Y\} \vdash Y$. This is usually called \textit{Modus Ponens}, and it is a valid argument form. In other words, for any choice of $X$, $Y$, it is true that $\{X, X \rightarrow Y\} \models Y$. So let's show the corresponding fact syntactically. 

As noted above, the first step here is to put the \textit{negation} of the conclusion at the root of the tree derivation. Since $Y$ is the conclusion, the tree will start out as such: 

\begin{center}
	\begin{prooftree}{}
		[{\neg Y}, just=Start]
	\end{prooftree}
\end{center}

Now unlike with tautologies, just having the negation of the conclusion gets us nowhere (if the conclusion is not itself a tautology). In fact, in this case, there is no rule you can use on $\neg Y$ to get further. On the other hand, what you \textit{can} do is use the premise rule. There are two premises, so you have a choice. It is not \textit{always} the case that all the premises are used, and in fact, the order in which they are used is also not predetermined, so there is a degree of freedom here. 

Let's start with the more complex premise:

\begin{center}
	\begin{prooftree}{}
		[{\neg Y}, just=Start
		[{X \rightarrow Y}, just=Pr.]
		]
	\end{prooftree}
\end{center}

Great! You should now see that you can use one of your other rules on the new formula. Thus: 

\begin{center}
	\begin{prooftree}{}
		[{\neg Y}, just=Start
		[{X \rightarrow Y}, just=Pr.
			[{\neg X}, just=$\rightarrow$:!u]
			[{Y}]
		]
		]
	\end{prooftree}
\end{center}

You may have immediately noticed that the right side branch closes by the occurrence of $Y$ and $\neg Y$ on it. Thus, we have: 

\begin{center}
	\begin{prooftree}{}
		[{\neg Y}, just=Start
		[{X \rightarrow Y}, just=Pr.
		[{\neg X}, just=$\rightarrow$:!u]
		[{Y}, close={:!uu,!c}]
		]
		]
	\end{prooftree}
\end{center}

On the other hand, you might also see that the left hand side branch of the tree remains open, and there are no formulas left on which we might apply a rule. Thus, we have to look for another premise. (Not) incidentally, our other premise, $X$ is just what we need. In particular:  

\begin{center}
	\begin{prooftree}{}
		[{\neg Y}, just=Start
		[{X \rightarrow Y}, just=Pr.
			[{\neg X}, just=$\rightarrow$:!u
			[{X}, just=Pr., close={:!u, !c}]
			]
			[{Y}, close={:!uu,!c}]
		]
		]
	\end{prooftree}
\end{center}

Accordingly, we can conclude that $\{X, X \rightarrow Y\} \vdash Y$. 

\begin{defn}
	Let $A=\{X_1, X_2, ...\}$ be any set of formulas and let $Y$ be any formula of $\mathcal{L}_0$. Then, the set of premises $A$ \textit{syntactically entails} $Y$, or $Y$ is a \textit{syntactic consequence} of $A$, \textit{iff} there is a closed tree with $\neg Y$ at its origin, and with possible uses of the rule Pr. with any member of $A$. In such cases, we write $A \vdash Y$, and call the closed tree a \textit{proof} of $Y$ from the premise set (or simply, premises) $A$. 
\end{defn}

Let's look at a more complicated proof before it is your turn. Suppose you want to evaluate the following argument:

\begin{center}
\begin{tabular}{ll}
(P1) & $X \vee \neg Y$ \\
(P2) & $\neg \neg Y$ \\
(C) & $(Q \vee Z) \rightarrow X$ 
\end{tabular}
\end{center}

As a first step, like before, we put the negation of the conclusion at the root of our tree. Thus: 

\begin{center}
	\begin{prooftree}{}
		[{\neg ((Q \vee Z) \rightarrow X)}, just=Start
			]
	\end{prooftree}
\end{center}

Note that we are negating the whole formula, so you have to provide the outer parentheses if they were omitted previously. 

It is a good idea to break this down a bit more immediately, since it won't branch. 

\begin{center}
	\begin{prooftree}{}
		[{\neg ((Q \vee Z) \rightarrow X)}, just=Start
			[{Q \vee Z}, just=$\neg\rightarrow$:!u
			[{\neg X}, just=$\neg\rightarrow$:!uu
		]
		]
		]	
	\end{prooftree}
\end{center}

As before, there are various ways to proceed, some shorter than others. In particular, we could start breaking down $Q \vee Z$. However, this would make the proof longer than necessary. Once you are more comfortable with how the system works, you will be able to see this. 

For now, let's proceed by adding $\neg \neg Y$ with our premise rule. We could immediately simplify it further, but for now, we will leave it as is. So we have:  

\begin{center}
	\begin{prooftree}{}
		[{\neg ((Q \vee Z) \rightarrow X)}, just=Start
		[{Q \vee Z}, just=$\neg\rightarrow$:!u
		[{\neg X}, just=$\neg\rightarrow$:!uu
		[{\neg \neg Y}, just=Pr.
		]
		]
		]
		]
		]	
	\end{prooftree}
\end{center}

Then, we can add the other premise to our tree, again, non-branching:

\begin{center}
	\begin{prooftree}{}
		[{\neg ((Q \vee Z) \rightarrow X)}, just=Start
		[{Q \vee Z}, just=$\neg\rightarrow$:!u
		[{\neg X}, just=$\neg\rightarrow$:!uu
		[{\neg \neg Y}, just=Pr.
		[{X \vee \neg Y}, just=Pr.]
		]
		]
		]
		]
		]	
	\end{prooftree}
\end{center}

We now have a choice to branch. But doing the proof briefly in our head, we immediately see that if we branch to $X$ and $\neg Y$, our tree immediately closes by $\neg X$ and $\neg \neg Y$. On the other hand, if we branch to $Q$ and $Z$, nothing happens, and we still need to branch further to $X$ and $\neg Y$ to close the tree. And in fact, in the latter case, we have to do it twice, resulting in four branches in the end. 

So the simplest way is to go immediately to: 

\begin{center}
	\begin{prooftree}{}
		[{\neg ((Q \vee Z) \rightarrow X)}, just=Start
		[{Q \vee Z}, just=$\neg\rightarrow$:!u
		[{\neg X}, just=$\neg\rightarrow$:!uu
		[{\neg \neg Y}, just=Pr.
		[{X \vee \neg Y}, just=Pr.
			[{X}, just=$\vee$:!u, close={:!uuu, !c}]
			[{\neg Y}, just=$\vee$:!u, close={:!uu, !c}]
		]
		]
		]
		]
		]
		]	
	\end{prooftree}
\end{center}

Again, you can compare this to: 

\begin{center}
	\begin{prooftree}{}
		[{\neg ((Q \vee Z) \rightarrow X)}, just=Start
		[{Q \vee Z}, just=$\neg\rightarrow$:!u
		[{\neg X}, just=$\neg\rightarrow$:!uu
		[{\neg \neg Y}, just=Pr.
		[{X \vee \neg Y}, just=Pr.
			[{Q}, just=$\vee$:!uuuu
				[{X}, just=$\vee$:!uu, close={:!uuuu, !c}]
				[{\neg Y}, close={:!uuu, !c}]
			]
			[{Z}
				[{X}, just=$\vee$:!uu, close={:!uuuu, !c}]
				[{\neg Y}, close={:!uuu, !c}]
			]
		]
		]
		]
		]
		]
		]	
	\end{prooftree}
\end{center}

And in fact, if you branched immediately at the beginning as many times as possible, and applied every rule as many times as possible, you would have had this:

\begin{center}
	\begin{prooftree}{}
		[{\neg ((Q \vee Z) \rightarrow X)}, just=Start
		[{Q \vee Z}, just=$\neg\rightarrow$:!u
			[{Q}, just=$\vee$:!u
				[{X \vee \neg Y}, just=Pr.
					[{X}, just=$\vee$:!u
						[{\neg X}, just=$\neg\rightarrow$:!uuuuu
						[{\neg \neg Y}, just=Pr.
						[{Y}, just=$\neg\neg$:!u, close={:!uuu, !uu}
						]
						]
						]
					]
					[{\neg Y}
						[{\neg X}
						[{\neg \neg Y}
						[{Y}, close={:!uuu,!c}
						]
						]
						]
					]
				]
			]
			[{Z}
				[{X \vee \neg Y}
					[{X}
						[{\neg X}
						[{\neg \neg Y}
						[{Y}, close={:!uuu, !uu}
						]
						]
						]
					]
					[{\neg Y}
						[{\neg X}
						[{\neg \neg Y}
						[{Y}, close={:!uuu,!c}
						]
						]
						]
					]
				]
			]
		]
		]
	\end{prooftree}
\end{center}

Now this may not look \textit{that} bad on paper, but notice that the first proof had 7 occurrences of formulas, the second had 11, while the last one had 22! On the other hand, none of these proofs are incorrect. It is just that they become less and less elegant.

\subsection{On soundness and completeness, again}

Note that we can now reformulate our soundness and completeness claims from above in a stronger form. Previously, we had:

\[
\models X \textit{ iff } \vdash X
\]

Again, from left to right, this is completeness, and from right to left, this is soundness. But we also have a stronger version of this, with multiple formulas, as opposed to singular ones. In particular:
%
\[
A \models X \textit{ iff } A \vdash X
\] 
%
Or in words, $A$ \textit{semantically} entails X \textit{iff} $A$ \textit{syntactically} entails X. And once again, from left to right, this is (strong) completeness, and from right to left, this is (strong) soundness (for zeroth-order logic). 

We shall not prove this biconditional in this book, as it involves more advanced techniques than we are ready for. However, we can note the following. 

Let's negate both sides of the biconditional again, just as before. Then, we get: 
%
\[
A \not\models X \textit{ iff } A \nvdash X
\] 
%
Again, we can try and unpack the two sides, and see what we get. First, if $A \nvdash X$, that means that $\neg X$ cannot have a closed tableau, no matter which members of $A$ are used in any of the attempted trees. In other words, all the trees starting with $\neg X$ will remain open, no matter how long they might be. 

On the other side, we have that $A \not\models X$, i.e., that $A$ does not semantically entail $X$. Now previously, we defined semantic entailment in terms of unsatisfiability. In particular, $A$ semantically entails $X$ provided the set $A$ together with $\neg X$ form a set (precisely $A$ plus $\neg X$) that is unsatisfiable. Accordingly, $A$ does \textit{not} semantically entail $X$ provided $A$ together with $\neg X$ is not unsatisfiable. Or in other words, $A$ together with $\neg X$ \textit{is} satisfiable.

Putting the above two results together, we then get that $A$ together with $\neg X$ is satisfiable \textit{iff} $\neg X$ does not have a closed tableau from premises $A$. 

\begin{exc}
For each set of formulas $A$ and formula $B$ of $\mathcal{L}_0$ below, show that $A \vdash B$ using the tableau method. 

\begin{enumerate}
	\item $A=\{X \rightarrow Y, Y \rightarrow Z, \neg Z\}$ and $B=\neg X$;
	\item $A=\{X \vee (Z \rightarrow Q), R \wedge Z, \neg Q\}$ and $B=X$;
	\item $A=\{\neg ((X \vee \neg Y) \wedge (Z \wedge \neg Q))\}$ and $B=(\neg X \wedge Y) \vee (\neg Z \vee Q)$
	\item $A=\{Z \rightarrow (X \wedge \neg X), \neg Q \vee Z, X \rightarrow Q\}$, $B=\neg X$;
	\item $A=\{(\neg X \wedge Y) \vee (Z \wedge \neg Q), \neg X \rightarrow \neg Y, Z \rightarrow Q\}$, $B=R$;
	\item $A=\{X \mid X\text{ is a formula of }\mathcal{L}_0\}$, $B=\neg Y$.
	\end{enumerate} 
\end{exc}
